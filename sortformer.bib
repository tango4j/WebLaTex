


@inproceedings{vaswani2017attention,
title={Attention is all you need},
author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
booktitle={Advances in Neural Information Processing Systems},
pages={5998--6008},
year={2017}
}


@article{brown2020gpt3,
title={GPT-3: Language Models are Few-Shot Learners},
author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subramanian, Mike and Kaplan, Jared and Dhariwal, Praveen and Neel, Dario and Shyam, Pranav and Mishkin, Adam and Radford, Alec},
journal={arXiv preprint arXiv:2001.08237},
year={2020}
}


@article{rao2022megatron,
  title={Megatron-turing nlg: Training an extensively labeled 530b parameter language model},
  author={Rao, Rajkrishna and Chen, Chenglong and Hendricks, Marcus and Kreutzer, Mathias and Li, Rasa and Mishkin, Michael and Price, Noah and Puri, Shafiq and Qadir, Muhammed and Reddi, Siva and Shobhana, Vikas and Talwar, Yash and Tay, Yen-Ling and Wang, Tong and Wei, Ming and Wilcox, Eric and Wu, Yu and Yuan, Yuan and Zhang, Pengcheng and Zhou, Yu},
  journal={arXiv preprint arXiv:2201.11991},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Niu, Kristina and Toutanova, Kristina and M. Belinkov, Yonatan and Peters, Michael and Nair, Vamsi and Poliakoff, Paul},
  booktitle={Advances in neural information processing systems},
  pages={9385--9404},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subramanian, Mukesh and Kaplan, Jared and Dhariwal, Praful and Agarwal, Nikhil and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2004.10151},
  year={2020}
}

@inproceedings{raffel2019learning,
  title={Learning to transfer the effectiveness of pretraining to downstream tasks},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Kim and Parekh, Nikita and Narasimhan, Karthik},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Jones, Llion and GÃ³mez, Aidan N and Polosukhin, Illia and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1802.05720},
  year={2018}
}

@inproceedings{carion2020equally,
  title={Equally effective object detection with deep convolutional networks and transformers},
  author={Carion, Nicolas and Tait, Kent and Giraudel, Brandon and Joulin, Armand and Laurent, Maxime and Douze, Mathieu},
  booktitle={European Conference on Computer Vision},
  pages={250--266},
  year={2020}
}

@article{dehghani2018universal,
  title={Universal transformers for machine translation},
  author={Dehghani, Mostafa and He, Shijian and Pourpreur, Nicolas and Liu, Yang},
  journal={arXiv preprint arXiv:1808.04044},
  year={2018}
}

@inproceedings{so2019hiredrop,
  title={Hiredrop: Learning conservative updates for long-range transformer models},
  author={So, Daniel and Li, Zhifeng and Yang, Cao and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{ahmed2017transformers,
  title={Grounded transformer networks for video action recognition},
  author={Ahmed, Farhan and Shelhamer, Erik and Darrell, Trevor},
  booktitle={IEEE International Conference on Computer Vision},
  pages={4595--4604},
  year={2017}
}

@inproceedings{kitaev2019reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12077--12090},
  year={2021}
}

@article{su2023roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  pages={127063},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International conference on machine learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{bloem2018neural,
  title={Neural network models of exchangeable sequences},
  author={Bloem-Reddy, Benjamin and Teh, Yee Whye},
  booktitle={NeurIPS Workshop on Bayesian Deep Learning},
  year={2018}
}

